{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e292b979-e7de-47e5-b6e6-f3795c44400b",
   "metadata": {},
   "source": [
    "## Geneious analysis for individual samples from raw Geneious output, \"Annotation.csv\" \n",
    "\n",
    " ### Required packages\n",
    " - No specific package required \n",
    " \n",
    " ### Inputs\n",
    " - Geneious SNP analysis of _k13_, _crt_, _mdr1_, _dhfr_, _dhps_, and _cytb_\n",
    " - Documentation on Geneious analysis can be found: Readme.md\n",
    " - Geneious outputs were modified to GuineaAnalysis_Individual.csv from \"Annotation.csv\"\n",
    " \n",
    " \n",
    " ### Data structure \n",
    " - [Long-form](https://seaborn.pydata.org/tutorial/data_structure.html#long-form-vs-wide-form-data) \n",
    "     - Each variable is a column \n",
    "\n",
    "         - \"Sample\" = *AMD ID*, including associated meta-data for each sample\n",
    "             - AMD ID and bit code key is found under MS Teams > Domestic > Files > Sample Naming > Sample_naming_key.pptx  \n",
    "\n",
    "             - Key: **Year Country State/Site DayofTreatment Treatment SampleID Genus SampleType GeneMarker-8bitcode SampleSeqCount**\n",
    "\n",
    "                 - Example:\n",
    "                     - Individual sequenced sample ID: 17GNDo00F0001PfF1290 = 2017 Guinea Dorota Day0 AS+AQ 0001 P.falciparum FilterBloodSpot k13-crt-mdr-dhfr-dhps-cytB-cpmp-pfs47 \n",
    "\n",
    "                     - Pooled sequenced sample ID: 17GNDoxxx001P10F1290 = 2017 Guinea Dorota **xx x** 001 **Pooled SamplesInPool** P.falciparum FilterBloodSpot k13-crt-mdr-dhfr-dhps-cytB-cpmp-pfs47 \n",
    "\n",
    "                         - NOTE: If information is not availble (na) **x** is used. For pooled samples, DayofTreatment and Treatment is na since its a pool of multiple samples with that info. \n",
    "                         - NOTE: For pooled samples, **Genus** is replaced with **Pooled** and **SampleType** with **SamplesInPool** to indicated this as a pooled sequenced sample and sample count in each pool. \n",
    "         <p>&nbsp;</p>\n",
    "         - \"Year\" = the year the study was conducted \n",
    "         - \"Site\" = the state or province \n",
    "         - \"Day_of_treatment\" = describes the day of treatment provided to the patient \n",
    "         - \"Gene\" = drug resistant gene(s) \n",
    "         - \"G_annotation\" = full SNP annotation in the following format: WildTypeAA-CodonPosition-MutantAA \n",
    "         - \"Coverage\" = the number of reads covering the SNP \n",
    "         - \"VAF\" = variant allele frequency calculated by AA divided by total reads in loci \n",
    "         - \"SNP\" = single nucleotide polymorphism in WildTypeAA or MutantAA annotation format \n",
    "         - \"Type\" = describes if it is a wild type or mutant SNP \n",
    "\n",
    "     - Each observation is a row for each sample ID (patient ID) \n",
    " \n",
    " #### TODO\n",
    " \n",
    " #### Activity Name\n",
    " - [ ] Write doc.string at the beginning of the code\n",
    " - [ ] Write detailed description with comment for line by line\n",
    " - [ ] Make the code more simple and accurate\n",
    " - [ ] Follow zen of python\n",
    "    \n",
    " #### Completed Activity âœ“\n",
    " - [x] Created marked down at the beginning of the file for description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "83bfdd5a-14d1-4bdd-bfac-48e4f7ac2261",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_117/2388402328.py:3: DtypeWarning: Columns (16,19,24,26,28,30,31,32,33,34,35,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,57,58,59,60,61,62,64,65,66,67,68) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Geneious_DF=pd.read_csv(\"Annotations.csv\") ##Import raw Geneious output for variant analysis\n",
      "/tmp/ipykernel_117/2388402328.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Geneious_DF_N1[\"TrackerSNP\"]=Geneious_DF_N1[\"Amino Acid Change\"].astype(str).str[0]+Geneious_DF_N1[\"CDS Codon Number\"].astype(int).astype(str)+Geneious_DF_N1[\"Amino Acid Change\"].astype(str).str[-1] ##Create a TrackerSNP Column which has both amino acid before the change and after the change\n",
      "/tmp/ipykernel_117/2388402328.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Combination_filtered[\"SITE\"]=Combination_filtered.apply(site, axis=1) ##Apply the functions defined previously\n",
      "/tmp/ipykernel_117/2388402328.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Combination_filtered[\"TreatmentDay\"]=Combination_filtered.apply(TreatmentDay, axis=1)\n",
      "/tmp/ipykernel_117/2388402328.py:62: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Combination_filtered[\"Pooled\"]=Combination_filtered.apply(Pooled, axis=1)\n",
      "/tmp/ipykernel_117/2388402328.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Combination_filtered[\"Year\"]=Combination_filtered.apply(year, axis=1)\n",
      "/tmp/ipykernel_117/2388402328.py:64: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Combination_filtered[\"TYPE\"]=Combination_filtered.apply(type, axis=1)\n",
      "/tmp/ipykernel_117/2388402328.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Combination_filtered[\"SNP\"]=Combination_filtered.apply(SNP, axis=1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd ## Import Pandas library for processing dataframe as pd\n",
    "import numpy as np ## Import Numy for processing matrix as np\n",
    "Geneious_DF=pd.read_csv(\"Annotations.csv\") ##Import raw Geneious output for variant analysis\n",
    "Geneious_DF_N1=Geneious_DF[(Geneious_DF['Type']=='Polymorphism') & (Geneious_DF['Amino Acid Change'].notnull())] ##If the type column contains polymorphism and Amino Acid Change column is not empty then create a dataframe satisfying those conditions\n",
    "\n",
    "Geneious_DF_N2=Geneious_DF[Geneious_DF['Type']=='Coverage - High'] ##If the Coverage - High is in the type column as value then select dataframe for those column values\n",
    "\n",
    "Geneious_DF_N1[\"TrackerSNP\"]=Geneious_DF_N1[\"Amino Acid Change\"].astype(str).str[0]+Geneious_DF_N1[\"CDS Codon Number\"].astype(int).astype(str)+Geneious_DF_N1[\"Amino Acid Change\"].astype(str).str[-1] ##Create a TrackerSNP Column which has both amino acid before the change and after the change\n",
    "\n",
    "Combine_Variant_Wildtpye = [Geneious_DF_N1, Geneious_DF_N2] ##Produce a complete dataframe which contains both variants and wildtypes\n",
    "Combation_Vi_Wi = pd.concat(Combine_Variant_Wildtpye) ##Concatenate the dataframes for variants and wildtypes\n",
    "Combination_filtered=Combation_Vi_Wi.drop_duplicates(subset =[\"Document Name\", \"TrackerSNP\"] )  ##Drop duplicates meaning if the values are already in variants then drop it from the wildtypes\n",
    "##The dataframe contains information, \"Sample,Pooled,Year,SITE,TreatmentDay,GENE,G_annotation,COVERAGE,VAF,VF,SNP,TYPE\\n\")\n",
    "        \n",
    "def site(row): ##Set up a function for assignging site based on the values in the document name column\n",
    "    if row['Document Name'][4:6]==\"Ha\":\n",
    "        return 'Hamdalaye'\n",
    "    elif row['Document Name'][4:6]==\"Do\":\n",
    "        return 'Dorota'\n",
    "    elif row['Document Name'][4:6]==\"Ma\":\n",
    "        return 'Maferinyah'\n",
    "    elif row['Document Name'][4:6]==\"La\":\n",
    "        return 'Lay-Sare'\n",
    "    elif row['Document Name'][4:6]==\"LS\":\n",
    "        return 'Lay-Sare'\n",
    "    \n",
    "def TreatmentDay(row): ##Set up a function for assigning TreatmentDay based on the values in the document name column\n",
    "    if row['Document Name'][6:8]==\"00\":\n",
    "        return '0'\n",
    "    elif row['Document Name'][6:8]==\"1A\":\n",
    "        return '1'\n",
    "    elif row['Document Name'][6:8]!=\"00\" and row['Document Name'][6:8]!=\"1A\":\n",
    "        return row['Document Name'][6:8]\n",
    "    \n",
    "def Pooled(row): ##Set up a function for Pooled based on the values in the document name column\n",
    "    if row['Document Name'][8:10]==\"xp\":\n",
    "        return 'pooled'\n",
    "    elif row['Document Name'][8:10]!=\"xp\":\n",
    "        return 'individual'\n",
    "\n",
    "def year(row):  ##Set up a function for Year  based on the values in the document name column\n",
    "    return row['Document Name'][0:2]\n",
    "\n",
    "def type(row):  ##Set up a TYPE column based on given value in the Type whether it is mutation or wildtype\n",
    "    if row['Type'] =='Polymorphism':\n",
    "        return \"mutation\"\n",
    "    if row['Type'] =='Coverage - High':\n",
    "        return \"wildtype\"\n",
    "    \n",
    "def SNP(row):  ##Set up a SNP column to give pre or post amino acid changes based on mutation or wildtype\n",
    "    if row['Type'] =='Polymorphism':\n",
    "        return row['TrackerSNP'][1::]\n",
    "    if row['Type'] =='Coverage - High':\n",
    "        return row['TrackerSNP'][0:-1]\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "Combination_filtered[\"SITE\"]=Combination_filtered.apply(site, axis=1) ##Apply the functions defined previously\n",
    "Combination_filtered[\"TreatmentDay\"]=Combination_filtered.apply(TreatmentDay, axis=1)\n",
    "Combination_filtered[\"Pooled\"]=Combination_filtered.apply(Pooled, axis=1)\n",
    "Combination_filtered[\"Year\"]=Combination_filtered.apply(year, axis=1)\n",
    "Combination_filtered[\"TYPE\"]=Combination_filtered.apply(type, axis=1)\n",
    "Combination_filtered[\"SNP\"]=Combination_filtered.apply(SNP, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "Combination_report1=Combination_filtered[Combination_filtered['Type']=='Polymorphism'] ##Select columns with mutations\n",
    "Combination_report2=Combination_filtered[Combination_filtered['Type']=='Coverage - High'] ##Select columns with wildtypes\n",
    "final_report1=Combination_report1[[\"Document Name\",\"Sequence Name\",\"SITE\",\"TreatmentDay\",\"Pooled\",\"Year\",\"Coverage\",\"Variant Frequency\",\"Variant Raw Frequency\",\"TrackerSNP\",\"TYPE\",\"SNP\"]] ##Assign sample information to samples with mutation\n",
    "final_report2=Combination_report2[[\"Document Name\",\"Sequence Name\",\"SITE\",\"TreatmentDay\",\"Pooled\",\"Year\",\"Average Coverage\",\"Variant Frequency\",\"Variant Raw Frequency\",\"TrackerSNP\",\"TYPE\",\"SNP\"]] ##Assign sample information to samples with wildtypes\n",
    "final_report2_re=final_report2.rename(columns={'Average Coverage': 'Coverage'}) ##Change the name of average coverage to coverage for samples with wildtypes\n",
    "final_combine=[final_report1, final_report2_re] ##Combine the information from wildtypes and mutations into one dataframe\n",
    "final_combine_2=pd.concat(final_combine) ##concatenate\n",
    "final_combine_2.to_csv(\"test.csv\", sep=',', index=False) ##Create a file with the dataframe for testing purpose\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bc46cb9f-c06f-435b-96cb-45ebd104f1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        100.0\n",
      "1        100.0\n",
      "2        100.0\n",
      "3        100.0\n",
      "4        100.0\n",
      "         ...  \n",
      "11444      NaN\n",
      "11445      NaN\n",
      "11446      NaN\n",
      "11447      NaN\n",
      "11448      NaN\n",
      "Name: Variant Frequency, Length: 11449, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_117/1774320522.py:19: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df_merged_poolsize = df_merged_poolsize.drop('SITE_y', 1) ##Get rid of duplicate columns which is SITE_y\n",
      "/tmp/ipykernel_117/1774320522.py:20: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df_merged_poolsize = df_merged_poolsize.drop('YEAR', 1) ##Get rid of duplicate columns which is year\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'test2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[0;32mIn [89]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_merged_poolsize[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVariant Frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mstr[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     28\u001b[0m df_merged_poolsize[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProd\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m=\u001b[39mdf_merged_poolsize[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVariant Frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mstr[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)\u001b[38;5;241m*\u001b[39mdf_merged_poolsize[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPoolsize\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m \u001b[43mdf_merged_poolsize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest2.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/generic.py:3563\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3552\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[1;32m   3554\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[1;32m   3555\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[1;32m   3556\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3560\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[1;32m   3561\u001b[0m )\n\u001b[0;32m-> 3563\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mline_terminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mline_terminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3566\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3568\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3570\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3572\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3573\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3574\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3575\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3576\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3577\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3578\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3579\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3580\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/formats/format.py:1180\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1159\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1161\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[1;32m   1162\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[1;32m   1163\u001b[0m     line_terminator\u001b[38;5;241m=\u001b[39mline_terminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[1;32m   1179\u001b[0m )\n\u001b[0;32m-> 1180\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[1;32m   1183\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/formats/csvs.py:241\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    249\u001b[0m \n\u001b[1;32m    250\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[1;32m    252\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[1;32m    253\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mline_terminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[1;32m    259\u001b[0m     )\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    788\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'test2.csv'"
     ]
    }
   ],
   "source": [
    "pooled_part1=pd.read_csv(\"Pooled_Info_Part1.csv\") ##import a file with information about pooled samples\n",
    "pooled_part2=pd.read_csv(\"Pooled_Info_Part2_fixed.csv\")  ##import a file with information about pooled samples\n",
    "\n",
    "Combine_pooled_parts = [pooled_part1[[\"Pool\",\"SITE\",\"YEAR\",\"AMD_ID\",\"Poolsize\"]], pooled_part2[[\"Pool\",\"SITE\",\"YEAR\",\"AMD_ID\",\"Poolsize\"]]]\n",
    "Combation_pooled_concatenate = pd.concat(Combine_pooled_parts) ##Combine and concatenate the two pooled files\n",
    "\n",
    "Combation_pooled_concatenate.to_csv(\"test-pre1.csv\", sep=',', index=False) ##create file for testing purpose\n",
    "\n",
    "def name(row):\n",
    "    return row['Document Name'].split(\"_\")[0]\n",
    "\n",
    "final_combine_2[\"Document Name\"]=final_combine_2.apply(name, axis=1) ##Clean the document name which is the AMD_ID to get rid of the Geneious information\n",
    "\n",
    "final_combine_2.rename(columns={'Document Name':'AMD_ID'}, inplace=True) ##Reanme the document name to AMD_ID\n",
    "\n",
    "\n",
    "df_merged_poolsize = pd.merge(final_combine_2, Combation_pooled_concatenate, on=['AMD_ID'], how='left') ##merge the information based on AMD_ID to add pooled columns and pooled size columns\n",
    "\n",
    "df_merged_poolsize = df_merged_poolsize.drop('SITE_y', 1) ##Get rid of duplicate columns which is SITE_y\n",
    "df_merged_poolsize = df_merged_poolsize.drop('YEAR', 1) ##Get rid of duplicate columns which is year\n",
    "\n",
    "df_merged_poolsize.Poolsize.fillna(value=1, inplace=True) ##Fill empty values for poolsize for individual with 1s\n",
    "\n",
    "df_merged_poolsize['Variant Frequency'] = df_merged_poolsize['Variant Frequency'].fillna(0)\n",
    "\n",
    "print(df_merged_poolsize[\"Variant Frequency\"].str.split('%').str[0].str.strip(\"%\"))\n",
    "\n",
    "df_merged_poolsize[\"Prod\"]=df_merged_poolsize[\"Variant Frequency\"].str.split('%').str[0].str.strip(\"%\").astype(float)*df_merged_poolsize[\"Poolsize\"].astype(float)\n",
    "\n",
    "df_merged_poolsize.to_csv(\"test2.csv\", sep=',', index=False) ##Generate file to test\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "fbdafdd2-4e85-441f-a444-7ae3318ad7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_117/3914089270.py:5: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df_merged_countv = df_merged_countv.drop('Pool', 1) ##drop pooled information\n",
      "/tmp/ipykernel_117/3914089270.py:51: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df_merged_countv = df_merged_countv.drop('Prod', 1) ##drop pooled information\n",
      "/tmp/ipykernel_117/3914089270.py:52: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df_merged_countv = df_merged_countv.drop('sum', 1) ##drop pooled information\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_merged_countis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [110]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m df_merged_countv\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtesttest3.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m#df_merged_countivsr=df_merged_countivs.reset_index()\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m#df_merged_countivsr2=df_merged_countivs.reset_index()\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m#df_merged_countivsr.drop(df_merged_countivsr.index[df_merged_countivsr['SNP'].str[0].str.isdigit()], inplace=True)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m#df_merged_countivsr_comb = df_merged_countivsr_comb.drop('Pool_y', 1)\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m#df_merged_countivsr_comb.to_csv(\"test5.csv\", sep=',', index=True)\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m \u001b[43mdf_merged_countis\u001b[49m\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSITE_x\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSITE\u001b[39m\u001b[38;5;124m'\u001b[39m}, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m#df_merged_countivsr.to_csv(\"test3.csv\", sep=',', index=True)\u001b[39;00m\n\u001b[1;32m     74\u001b[0m df_merged_countpool \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(df_merged_counti, Combation_pooled_concatenate, on\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSITE\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrackerSNP\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPooled\u001b[39m\u001b[38;5;124m'\u001b[39m], how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m) \n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_merged_countis' is not defined"
     ]
    }
   ],
   "source": [
    "df_merged_count=df_merged_poolsize.groupby(['SITE_x','TrackerSNP', 'Pooled']).sum()  ##Sum the columns based on overlapping values on site, trackersnp, and pooled\n",
    "df_merged_countv=df_merged_poolsize.groupby(['SITE_x','TrackerSNP', 'Pooled', 'SNP']).sum() ##Sum the columns based on agreeing values on site, trackersnp, and pooled, and snp\n",
    "df_merged_count=df_merged_count.groupby(['SITE_x','TrackerSNP']).sum() ##Sum again based on stie and tracker snp\n",
    "df_merged_countv=df_merged_countv.groupby(['SITE_x','TrackerSNP','SNP']).sum() ##Sum again based on site, tracker, and snp\n",
    "df_merged_countv = df_merged_countv.drop('Pool', 1) ##drop pooled information\n",
    "\n",
    "#df_merged_countv.to_csv(\"testtest1.csv\", sep=',', index=True)\n",
    "\n",
    "df_merged_countv=df_merged_countv.reset_index()  \n",
    "df_merged_countv['Type'] = np.where(df_merged_countv['SNP'].str[0].str.isdigit(), \"Mutation\" , \"WildType\")\n",
    "df_merged_countv.rename(columns={'Poolsize':'Number_of_samples'}, inplace=True)\n",
    "df_merged_countvp=df_merged_countv\n",
    "df_merged_countv=df_merged_countv.pivot(index=[\"SITE_x\", \"TrackerSNP\"], columns=\"Type\", values=\"Number_of_samples\") ##pivot and align mutation and wildtype\n",
    "df_merged_countvp=df_merged_countvp.pivot(index=[\"SITE_x\", \"TrackerSNP\"], columns=\"Type\", values=\"Prod\") ##pivot and align mutation and wildtype\n",
    "\n",
    "#df_merged_countvp.to_csv(\"testtest1.csv\", sep=',', index=True)\n",
    "\n",
    "\"\"\"\n",
    "To do: \n",
    "https://pandas.pydata.org/pandas-docs/stable/user_guide/reshaping.html\n",
    "This is where I am stuck. Trying using pivot to fix it\n",
    "\"\"\"\n",
    "\n",
    "df_merged_countvp=df_merged_countvp.reset_index()\n",
    "df_merged_countv=df_merged_countv.reset_index()  \n",
    "\n",
    "df_merged_countv['SNP'] = np.where(pd.isna(df_merged_countv['Mutation']), df_merged_countv['TrackerSNP'].astype(str).str[0:-1], df_merged_countv['TrackerSNP'].astype(str).str[1::])\n",
    "\n",
    "cols = list(df_merged_countv.columns)\n",
    "cols = cols[0:2] + cols[4:5] + cols[2:4]\n",
    "df_merged_countv = df_merged_countv[cols]\n",
    "df_merged_countv.rename(columns={'TrackerSNP':'G_Annotation'}, inplace=True)\n",
    "\n",
    "df_merged_countv['Prod']= df_merged_countvp['Mutation']\n",
    "\n",
    "df_merged_countv['Prod'] = df_merged_countv['Prod'].fillna(0)\n",
    "\n",
    "df_merged_countv['Mutation'] = df_merged_countv['Mutation'].fillna(0)\n",
    "\n",
    "df_merged_countv['WildType'] = df_merged_countv['WildType'].fillna(0)\n",
    "\n",
    "df_merged_countv['sum'] = df_merged_countv['Mutation'] + df_merged_countv['WildType']\n",
    "\n",
    "df_merged_countv['div'] = df_merged_countv['Prod'] / df_merged_countv['sum']\n",
    "\n",
    "df_merged_countv['div']=df_merged_countv['div'].round(2)\n",
    "\n",
    "df_merged_countv['div']=np.where(pd.isna(df_merged_countv['div']),\"\", df_merged_countv['div'].astype(str)+\"%\") \n",
    "#df_merged_countv['div'].astype(str)+\"%\"\n",
    "\n",
    "df_merged_countv = df_merged_countv.drop('Prod', 1) ##drop pooled information\n",
    "df_merged_countv = df_merged_countv.drop('sum', 1) ##drop pooled information\n",
    "\n",
    "df_merged_countv.rename(columns={'div':'VAF'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "df_merged_countv.to_csv(\"testtest3.csv\", sep=',', index=True)\n",
    "#df_merged_countivsr=df_merged_countivs.reset_index()\n",
    "#df_merged_countivsr2=df_merged_countivs.reset_index()\n",
    "#df_merged_countivsr.drop(df_merged_countivsr.index[df_merged_countivsr['SNP'].str[0].str.isdigit()], inplace=True)\n",
    "#df_merged_countivsr2.drop(df_merged_countivsr.index[df_merged_countivsr['SNP'].str[0].str.isdigit()==False], inplace=True)\n",
    "#df_merged_countivsr2.to_csv(\"test3.csv\", sep=',', index=True)\n",
    "#df_merged_countivsr.to_csv(\"test4.csv\", sep=',', index=True)\n",
    "#df_merged_countivsr_comb = pd.merge(df_merged_count, df_merged_countv, on=['SITE_x','TrackerSNP'], how='left')\n",
    "#df_merged_countivsr_comb = df_merged_countivsr_comb.drop('Pool_x', 1)\n",
    "#df_merged_countivsr_comb = df_merged_countivsr_comb.drop('Pool_y', 1)\n",
    "#df_merged_countivsr_comb.to_csv(\"test5.csv\", sep=',', index=True)\n",
    "\n",
    "\n",
    "\n",
    "df_merged_countis.rename(columns={'SITE_x':'SITE'}, inplace=True)\n",
    "#df_merged_countivsr.to_csv(\"test3.csv\", sep=',', index=True)\n",
    "df_merged_countpool = pd.merge(df_merged_counti, Combation_pooled_concatenate, on=['SITE','TrackerSNP','Pooled'], how='left') \n",
    "#df_merged_countpool.to_csv(\"test3.csv\", sep=',', index=False)\n",
    "df_merged_countpool.Poolsize_y.fillna(value=1, inplace=True)\n",
    "df_merged_countpool_filtered = df_merged_countpool[['AMD_ID','Sequence Name','Poolsize_y']]\n",
    "df_merged_countpool_filtered.rename(columns={'Sequence Name':'Sequence_Name'}, inplace=True)\n",
    "df_merged_countpool_filtered.to_csv(\"test4.csv\", sep=',', index=False)\n",
    "values = df_merged_countpool_filtered.Sequence_Name * df_merged_countpool_filtered.Poolsize_y\n",
    "df_merged_countpool_filtered['values']=values\n",
    "#print(df_merged_countpool_filtered)\n",
    "df_merged_countpool_filtered.to_csv(\"test5.csv\", sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9217d2-e75e-430f-9343-ea5698d4c222",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_poolsize[]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
